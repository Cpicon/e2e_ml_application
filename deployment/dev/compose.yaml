services:
  # This service runs the postgres DB used by dagster for run storage, schedule storage,
  # and event log storage.
  dagster_postgresql:
    image: postgres
    container_name: dagster_postgresql
    environment:
      POSTGRES_USER: "postgres_user"
      POSTGRES_PASSWORD: "postgres_password"
      POSTGRES_DB: "postgres_db"
    healthcheck:
      test: [ "CMD", "pg_isready", "-p", "5432", "-U", "postgres_user" ]
      interval: 5s
      timeout: 5s
      retries: 3
    networks:
      - project_network

  # This service runs the gRPC server that loads your user code, in both dagster-webserver
  # and dagster-daemon. By setting DAGSTER_CURRENT_IMAGE to its own image, we tell the
  # run launcher to use this same image when launching runs in a new container as well.
  # Multiple containers like this can be deployed separately - each just needs to run on
  # its own port, and have its own entry in the workspace.yaml file that's loaded by the
      # webserver.
  dagster_user_code:
    build:
      context: ../..
      dockerfile: deployment/dev/Dockerfile_dagster_user_code
    container_name: Dockerfile_dagster_user_code
    image: dagster_user_code_image:latest
    restart: always
#    entrypoint: tail -f /dev/null
    environment:
      DAGSTER_POSTGRES_USER: "postgres_user"
      DAGSTER_POSTGRES_PASSWORD: "postgres_password"
      DAGSTER_POSTGRES_DB: "postgres_db"
      DAGSTER_CURRENT_IMAGE: "dagster_user_code_image"
      MLFLOW_TRACKING_URI : ${MLFLOW_TRACKING_URI}
      MLFLOW_S3_ENDPOINT_URL : ${MLFLOW_S3_ENDPOINT_URL}
      AWS_ACCESS_KEY_ID : ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY : ${AWS_SECRET_ACCESS_KEY}
    networks:
      - project_network

  # This service runs dagster-webserver, which loads your user code from the user code container.
  # Since our instance uses the QueuedRunCoordinator, any runs submitted from the webserver will be put on
  # a queue and later dequeued and launched by dagster-daemon.
  dagster_webserver:
    build:
      context: ../..
      dockerfile: deployment/dev/Dockerfile_dagster_base
    entrypoint:
      - dagster-webserver
      - -h
      - "0.0.0.0"
      - -p
      - "3000"
      - -w
      - workspace.yaml
    container_name: dagster_webserver
    image: dagster_image:latest
    expose:
      - "3000"
    ports:
      - "3000:3000"
    environment:
      DAGSTER_POSTGRES_USER: "postgres_user"
      DAGSTER_POSTGRES_PASSWORD: "postgres_password"
      DAGSTER_POSTGRES_DB: "postgres_db"
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    volumes: # Make docker client accessible so we can terminate containers from the webserver
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
    depends_on:
      dagster_postgresql:
         condition: service_healthy
      dagster_user_code:
         condition: service_started
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:3000/server_info" ]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - project_network


  # This service runs the dagster-daemon process, which is responsible for taking runs
  # off of the queue and launching them, as well as creating runs from schedules or sensors.
  dagster_daemon:
    entrypoint:
      - dagster-daemon
      - run
    container_name: dagster_daemon
    image: dagster_image:latest
    restart: on-failure
    environment:
      DAGSTER_POSTGRES_USER: "postgres_user"
      DAGSTER_POSTGRES_PASSWORD: "postgres_password"
      DAGSTER_POSTGRES_DB: "postgres_db"
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    volumes: # Make docker client accessible so we can launch containers using host docker
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - dagster_artifact_storage:/opt/dagster/app/data/MNIST/train/
      - dagster_artifact_storage:/opt/dagster/app/data/MNIST/test/
      - dagster_artifact_storage:/opt/dagster/app/data/MNIST/raw/
    depends_on:
      - dagster_postgresql
      - dagster_user_code
    networks:
      - project_network

  minio:
      restart: always
      image: minio/minio
      container_name: minio
      ports:
          - "9000:9000"
          - "9001:9001"
      command: server /data --address ':9000'
      environment:
          MINIO_ROOT_USER : ${AWS_ACCESS_KEY_ID}
          MINIO_ROOT_PASSWORD : ${AWS_SECRET_ACCESS_KEY}
          MINIO_VOLUMES: "/data"
          MINIO_STORAGE_USE_HTTPS: False
          MINIO_CONSOLE_ADDRESS: ${MINIO_CONSOLE_ADDRESS}
          MINIO_PORT: ${MINIO_PORT}
          MINIO_ACCESS_KEY: ${AWS_ACCESS_KEY_ID}
          MINIO_SECRET_KEY: ${AWS_SECRET_ACCESS_KEY}
      volumes:
          - minio_data:/data
      healthcheck:
          test: [ "CMD", "mc", "ready", "local" ]
          interval: 5s
          timeout: 5s
          retries: 5
      networks:
        - project_network

  mc:
      image: minio/mc
      depends_on:
          minio:
              condition: service_started
      container_name: mc
      env_file:
          - .env
      entrypoint: >
          /bin/sh -c "
          /tmp/wait-for-it.sh minio:9000 &&
          /usr/bin/mc alias set minio http://minio:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY} &&
          /usr/bin/mc config host add mlflow http://minio:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY} &&
          /usr/bin/mc mb minio/mlflow;
          exit 0;
          "
      volumes:
          - ./wait-for-it.sh:/tmp/wait-for-it.sh
      networks:
        - project_network

  debug:
      image: busybox
      container_name: debug
      command: tail -f /dev/null
      networks:
        - project_network

  mlflow_postgresql:
      restart: always
      image: postgres:latest
      container_name: mlflow_postgresql
      environment:
          POSTGRES_USER: ${MLFLOW_PG_USER}
          POSTGRES_PASSWORD: ${MLFLOW_PG_PWD}
          POSTGRES_DB: ${MLFLOW_DB}
      command: -p 5433
      volumes:
          - dbdata:/var/lib/postgresql/data
      ports:
          - "5433:5433"
      healthcheck:
          test: [ "CMD", "pg_isready", "-p", "5433", "-U", "${MLFLOW_PG_USER}" ]
          interval: 5s
          timeout: 5s
          retries: 3
      networks:
        - project_network

  mlflow_tracking_server:
      restart: always
      build:
          context: ../..
          dockerfile: deployment/dev/Dockerfile_mlflow
      image: mlflow_tracking_server
      container_name: mlflow_tracking_server
      ports:
          - "5005:5005"
      environment:
          MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
          MLFLOW_S3_IGNORE_TLS: "True"
          MLFLOW_DB_BACKEND_URI: postgresql://${MLFLOW_PG_USER}:${MLFLOW_PG_PWD}@mlflow_postgresql:5433/${MLFLOW_DB}
          AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
          AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      entrypoint: mlflow server --backend-store-uri postgresql://${MLFLOW_PG_USER}:${MLFLOW_PG_PWD}@mlflow_postgresql:5433/${MLFLOW_DB} --default-artifact-root s3://mlflow/ --serve-artifacts --serve-artifacts --host 0.0.0.0:5005
      healthcheck:
          test: [ "CMD", "curl", "-f", "http://localhost:5005" ]
          interval: 30s
          timeout: 10s
          retries: 3
      networks:
        - project_network
      depends_on:
          mlflow_postgresql:
              condition: service_healthy
          mc:
              condition: service_completed_successfully

volumes:
    dbdata:
    minio_data:
    dagster_artifact_storage:

networks:
  project_network:
    driver: bridge
    name: project_network